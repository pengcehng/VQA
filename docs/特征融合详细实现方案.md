# VQA模型特征融合详细实现方案

本文档提供了更换网络后特征融合的详细代码实现，包括多种融合策略的完整实现方案。

## 1. 基础架构设计

### 1.1 特征提取器基类

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from abc import ABC, abstractmethod

class BaseFeatureExtractor(nn.Module, ABC):
    """特征提取器基类"""
    
    def __init__(self, output_dim=512):
        super().__init__()
        self.output_dim = output_dim
    
    @abstractmethod
    def forward(self, x):
        """前向传播"""
        pass
    
    @abstractmethod
    def get_feature_dims(self):
        """获取特征维度信息"""
        pass

class EfficientNetExtractor(BaseFeatureExtractor):
    """EfficientNet特征提取器
    
    EfficientNet采用复合缩放策略，通过同时优化网络深度、宽度和分辨率，
    实现最优的性能-效率平衡。核心创新包括：
    1. 移动倒置瓶颈结构(MBConv)：结合深度可分离卷积和SE注意力
    2. 神经架构搜索(NAS)优化：自动搜索最优网络结构
    3. 复合缩放法则：depth × width × resolution = 2^φ
    """
    
    def __init__(self, model_name='efficientnet-b3', output_dim=512, use_se=True, drop_rate=0.2):
        super().__init__(output_dim)
        from efficientnet_pytorch import EfficientNet
        
        # 模型规格配置
        self.model_configs = {
            'efficientnet-b0': {'params': '5.3M', 'input_size': 224, 'feature_dim': 1280},
            'efficientnet-b3': {'params': '12M', 'input_size': 300, 'feature_dim': 1536},
            'efficientnet-b7': {'params': '66M', 'input_size': 600, 'feature_dim': 2560}
        }
        
        self.model_name = model_name
        self.config = self.model_configs[model_name]
        
        # 加载预训练模型
        self.backbone = EfficientNet.from_pretrained(model_name)
        self.backbone._fc = nn.Identity()  # 移除分类头
        
        # 获取backbone输出维度
        backbone_dim = self.config['feature_dim']
        
        # 多层特征投影
        self.projection = nn.Sequential(
            nn.Linear(backbone_dim, backbone_dim // 2),
            nn.BatchNorm1d(backbone_dim // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(drop_rate),
            nn.Linear(backbone_dim // 2, output_dim),
            nn.BatchNorm1d(output_dim)
        )
        
        # SE注意力模块（可选）
        self.use_se = use_se
        if use_se:
            self.se_module = nn.Sequential(
                nn.AdaptiveAvgPool1d(1),
                nn.Linear(output_dim, output_dim // 16),
                nn.ReLU(inplace=True),
                nn.Linear(output_dim // 16, output_dim),
                nn.Sigmoid()
            )
        
        # 多尺度池化
        self.multi_scale_pool = nn.ModuleList([
            nn.AdaptiveAvgPool2d(1),  # 全局平均池化
            nn.AdaptiveMaxPool2d(1),  # 全局最大池化
        ])
        
    def forward(self, x):
        # 提取多层特征
        features = self.backbone.extract_features(x)  # [B, C, H, W]
        
        # 多尺度池化
        pooled_features = []
        for pool in self.multi_scale_pool:
            pooled = pool(features).flatten(1)  # [B, C]
            pooled_features.append(pooled)
        
        # 拼接多尺度特征
        combined_features = torch.cat(pooled_features, dim=1)  # [B, 2*C]
        
        # 降维到原始维度
        combined_features = nn.Linear(combined_features.size(1), features.size(1)).to(x.device)(combined_features)
        
        # 特征投影
        projected_features = self.projection(combined_features)
        
        # SE注意力（可选）
        if self.use_se:
            se_weights = self.se_module(projected_features.unsqueeze(-1)).squeeze(-1)
            projected_features = projected_features * se_weights
        
        return projected_features
    
    def get_feature_dims(self):
        return {
            'output': self.output_dim,
            'backbone_dim': self.config['feature_dim'],
            'input_size': self.config['input_size'],
            'params': self.config['params']
        }
    
    def get_model_info(self):
        """获取模型详细信息"""
        return {
            'architecture': 'EfficientNet',
            'model_name': self.model_name,
            'scaling_strategy': 'Compound Scaling (depth × width × resolution)',
            'key_innovations': [
                'Mobile Inverted Bottleneck Convolution (MBConv)',
                'Squeeze-and-Excitation (SE) attention',
                'Neural Architecture Search (NAS) optimization',
                'Compound scaling method'
            ],
            'advantages': [
                'Superior parameter efficiency',
                'Strong transfer learning capability',
                'Balanced accuracy-efficiency trade-off',
                'Mobile-friendly design'
            ]
        }

class ViTExtractor(BaseFeatureExtractor):
    """Vision Transformer特征提取器
    
    Vision Transformer革命性地将Transformer架构引入计算机视觉，
    通过将图像分割为patches并视为序列进行处理。核心创新包括：
    1. 图像分块(Patch Embedding)：将2D图像转换为1D序列
    2. 位置编码(Position Embedding)：为每个patch添加位置信息
    3. 多头自注意力机制：捕获全局空间依赖关系
    4. 分层特征学习：通过多层Transformer编码器逐步抽象特征
    """
    
    def __init__(self, model_name='vit_base_patch16_224', output_dim=512, 
                 num_heads=12, mlp_ratio=4.0, drop_rate=0.1, attn_drop_rate=0.1):
        super().__init__(output_dim)
        import timm
        
        # 模型规格配置
        self.model_configs = {
            'vit_tiny_patch16_224': {
                'params': '5.7M', 'embed_dim': 192, 'depth': 12, 
                'num_heads': 3, 'patch_size': 16, 'input_size': 224
            },
            'vit_small_patch16_224': {
                'params': '22M', 'embed_dim': 384, 'depth': 12,
                'num_heads': 6, 'patch_size': 16, 'input_size': 224
            },
            'vit_base_patch16_224': {
                'params': '86M', 'embed_dim': 768, 'depth': 12,
                'num_heads': 12, 'patch_size': 16, 'input_size': 224
            },
            'vit_large_patch16_224': {
                'params': '307M', 'embed_dim': 1024, 'depth': 24,
                'num_heads': 16, 'patch_size': 16, 'input_size': 224
            }
        }
        
        self.model_name = model_name
        self.config = self.model_configs[model_name]
        
        # 加载预训练模型
        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)
        
        # 获取backbone输出维度
        backbone_dim = self.config['embed_dim']
        
        # 多层特征投影网络
        self.projection = nn.Sequential(
            nn.LayerNorm(backbone_dim),
            nn.Linear(backbone_dim, backbone_dim // 2),
            nn.GELU(),
            nn.Dropout(drop_rate),
            nn.Linear(backbone_dim // 2, output_dim),
            nn.LayerNorm(output_dim)
        )
        
        # 多尺度注意力池化
        self.attention_pool = nn.MultiheadAttention(
            embed_dim=output_dim,
            num_heads=num_heads // 2,
            dropout=attn_drop_rate,
            batch_first=True
        )
        
        # 可学习的查询向量
        self.query_tokens = nn.Parameter(torch.randn(1, 4, output_dim))
        
        # 位置编码增强
        self.pos_encoding = nn.Parameter(
            torch.randn(1, (self.config['input_size'] // self.config['patch_size']) ** 2 + 1, backbone_dim)
        )
        
    def forward(self, x):
        # 获取patch embeddings和位置编码
        batch_size = x.size(0)
        
        # 提取特征（包含CLS token）
        features = self.backbone.forward_features(x)  # [B, N+1, D]
        
        # 分离CLS token和patch tokens
        cls_token = features[:, 0]  # [B, D]
        patch_tokens = features[:, 1:]  # [B, N, D]
        
        # 特征投影
        projected_cls = self.projection(cls_token)  # [B, output_dim]
        projected_patches = self.projection(patch_tokens)  # [B, N, output_dim]
        
        # 多尺度注意力池化
        query_tokens = self.query_tokens.expand(batch_size, -1, -1)  # [B, 4, output_dim]
        attended_features, _ = self.attention_pool(
            query_tokens, projected_patches, projected_patches
        )  # [B, 4, output_dim]
        
        # 融合CLS token和注意力池化特征
        pooled_features = attended_features.mean(dim=1)  # [B, output_dim]
        final_features = (projected_cls + pooled_features) / 2
        
        return final_features
    
    def get_feature_dims(self):
        return {
            'output': self.output_dim,
            'embed_dim': self.config['embed_dim'],
            'depth': self.config['depth'],
            'num_heads': self.config['num_heads'],
            'patch_size': self.config['patch_size'],
            'input_size': self.config['input_size'],
            'params': self.config['params']
        }
    
    def get_model_info(self):
        """获取模型详细信息"""
        return {
            'architecture': 'Vision Transformer (ViT)',
            'model_name': self.model_name,
            'core_mechanism': 'Self-Attention on Image Patches',
            'key_innovations': [
                'Patch-based image tokenization',
                'Learnable position embeddings',
                'Global self-attention mechanism',
                'CLS token for classification',
                'Layer normalization and residual connections'
            ],
            'advantages': [
                'Global receptive field from first layer',
                'Strong scalability with data and compute',
                'Excellent transfer learning capability',
                'Interpretable attention patterns',
                'No inductive biases (translation equivariance)'
            ],
            'architectural_details': {
                'patch_embedding': f"{self.config['patch_size']}x{self.config['patch_size']} patches",
                'sequence_length': (self.config['input_size'] // self.config['patch_size']) ** 2 + 1,
                'transformer_layers': self.config['depth'],
                'attention_heads': self.config['num_heads'],
                'embedding_dimension': self.config['embed_dim']
            }
        }

class ConvNeXtExtractor(BaseFeatureExtractor):
    """ConvNeXt特征提取器
    
    ConvNeXt是现代化的纯卷积网络，通过借鉴Vision Transformer的设计原则
    来改进传统ResNet架构。核心创新包括：
    1. 深度可分离卷积：减少参数量和计算复杂度
    2. 倒置瓶颈设计：先扩展后压缩的特征变换
    3. LayerNorm替代BatchNorm：更稳定的训练过程
    4. GELU激活函数：更平滑的非线性变换
    5. 更少的激活函数：减少信息损失
    """
    
    def __init__(self, model_name='convnext_base', output_dim=512, 
                 drop_path_rate=0.1, layer_scale_init_value=1e-6):
        super().__init__(output_dim)
        import timm
        
        # 模型规格配置
        self.model_configs = {
            'convnext_tiny': {
                'params': '28.6M', 'embed_dims': [96, 192, 384, 768],
                'depths': [3, 3, 9, 3], 'input_size': 224
            },
            'convnext_small': {
                'params': '50.2M', 'embed_dims': [96, 192, 384, 768],
                'depths': [3, 3, 27, 3], 'input_size': 224
            },
            'convnext_base': {
                'params': '88.6M', 'embed_dims': [128, 256, 512, 1024],
                'depths': [3, 3, 27, 3], 'input_size': 224
            },
            'convnext_large': {
                'params': '197.8M', 'embed_dims': [192, 384, 768, 1536],
                'depths': [3, 3, 27, 3], 'input_size': 224
            }
        }
        
        self.model_name = model_name
        self.config = self.model_configs[model_name]
        
        # 加载预训练模型
        self.backbone = timm.create_model(
            model_name, 
            pretrained=True, 
            num_classes=0,
            drop_path_rate=drop_path_rate
        )
        
        # 获取backbone输出维度
        backbone_dim = self.config['embed_dims'][-1]  # 最后一层的维度
        
        # 多尺度特征融合
        self.feature_fusion = nn.ModuleDict({
            'stage1': nn.Conv2d(self.config['embed_dims'][0], output_dim // 4, 1),
            'stage2': nn.Conv2d(self.config['embed_dims'][1], output_dim // 4, 1),
            'stage3': nn.Conv2d(self.config['embed_dims'][2], output_dim // 2, 1),
            'stage4': nn.Conv2d(self.config['embed_dims'][3], output_dim, 1)
        })
        
        # 自适应池化层
        self.adaptive_pools = nn.ModuleDict({
            'avg_pool': nn.AdaptiveAvgPool2d(1),
            'max_pool': nn.AdaptiveMaxPool2d(1),
            'attention_pool': nn.Sequential(
                nn.Conv2d(backbone_dim, backbone_dim // 8, 1),
                nn.ReLU(inplace=True),
                nn.Conv2d(backbone_dim // 8, 1, 1),
                nn.Sigmoid()
            )
        })
        
        # 特征投影网络
        self.projection = nn.Sequential(
            nn.LayerNorm(backbone_dim),
            nn.Linear(backbone_dim, backbone_dim // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(backbone_dim // 2, output_dim),
            nn.LayerNorm(output_dim)
        )
        
        # 层级缩放参数
        self.layer_scale = nn.Parameter(
            layer_scale_init_value * torch.ones(output_dim)
        )
        
    def forward(self, x):
        # 提取多阶段特征
        features = self.backbone.forward_features(x)  # 最终特征 [B, C, H, W]
        
        # 多尺度池化
        avg_pooled = self.adaptive_pools['avg_pool'](features).flatten(1)  # [B, C]
        max_pooled = self.adaptive_pools['max_pool'](features).flatten(1)  # [B, C]
        
        # 注意力池化
        attention_weights = self.adaptive_pools['attention_pool'](features)  # [B, 1, H, W]
        attention_pooled = (features * attention_weights).sum(dim=[2, 3])  # [B, C]
        
        # 融合多种池化结果
        combined_features = (avg_pooled + max_pooled + attention_pooled) / 3
        
        # 特征投影
        projected_features = self.projection(combined_features)
        
        # 应用层级缩放
        scaled_features = projected_features * self.layer_scale
        
        return scaled_features
    
    def get_multi_scale_features(self, x):
        """获取多尺度特征（用于特征金字塔）"""
        # 需要修改backbone以输出中间特征
        # 这里提供接口，实际实现需要根据具体需求调整
        stage_features = []
        
        # 模拟多尺度特征提取
        with torch.no_grad():
            features = self.backbone.forward_features(x)
            
        # 返回不同尺度的特征
        return {
            'stage1': features,  # 1/4 分辨率
            'stage2': features,  # 1/8 分辨率  
            'stage3': features,  # 1/16 分辨率
            'stage4': features   # 1/32 分辨率
        }
    
    def get_feature_dims(self):
        return {
            'output': self.output_dim,
            'backbone_dim': self.config['embed_dims'][-1],
            'stage_dims': self.config['embed_dims'],
            'depths': self.config['depths'],
            'input_size': self.config['input_size'],
            'params': self.config['params']
        }
    
    def get_model_info(self):
        """获取模型详细信息"""
        return {
            'architecture': 'ConvNeXt',
            'model_name': self.model_name,
            'design_philosophy': 'Modernized ConvNet inspired by Vision Transformers',
            'key_innovations': [
                'Depthwise separable convolutions',
                'Inverted bottleneck design (4x expansion ratio)',
                'LayerNorm instead of BatchNorm',
                'GELU activation function',
                'Fewer activation functions',
                'Layer scale for training stability',
                'Stochastic depth for regularization'
            ],
            'advantages': [
                'Pure convolutional architecture',
                'Strong inductive biases for vision tasks',
                'Efficient parameter utilization',
                'Excellent transfer learning performance',
                'Competitive with Vision Transformers',
                'Better computational efficiency'
            ],
            'architectural_details': {
                'stage_depths': self.config['depths'],
                'embedding_dimensions': self.config['embed_dims'],
                'total_parameters': self.config['params'],
                'kernel_sizes': '7x7 (stem), 3x3 (depthwise)',
                'normalization': 'LayerNorm',
                'activation': 'GELU'
            }
        }
```

### 1.2 多尺度特征提取器

```python
class MultiScaleFeatureExtractor(nn.Module):
    """多尺度特征提取器"""
    
    def __init__(self, backbone_type='efficientnet', scales=[224, 384, 512]):
        super().__init__()
        self.scales = scales
        
        # 创建不同尺度的特征提取器
        if backbone_type == 'efficientnet':
            self.extractors = nn.ModuleList([
                EfficientNetExtractor(output_dim=512) for _ in scales
            ])
        elif backbone_type == 'vit':
            self.extractors = nn.ModuleList([
                ViTExtractor(output_dim=512) for _ in scales
            ])
        elif backbone_type == 'convnext':
            self.extractors = nn.ModuleList([
                ConvNeXtExtractor(output_dim=512) for _ in scales
            ])
        
        # 特征融合层
        self.fusion_layer = nn.Linear(512 * len(scales), 512)
        
    def forward(self, x):
        batch_size = x.size(0)
        multi_scale_features = []
        
        for i, scale in enumerate(self.scales):
            # 调整输入尺寸
            scaled_x = F.interpolate(x, size=(scale, scale), mode='bilinear', align_corners=False)
            # 提取特征
            features = self.extractors[i](scaled_x)
            multi_scale_features.append(features)
        
        # 拼接多尺度特征
        combined_features = torch.cat(multi_scale_features, dim=1)
        # 融合特征
        fused_features = self.fusion_layer(combined_features)
        
        return fused_features
```

## 2. 注意力融合机制

### 2.1 交叉模态注意力

```python
class CrossModalAttention(nn.Module):
    """交叉模态注意力机制"""
    
    def __init__(self, feature_dim=512, num_heads=8, dropout=0.1):
        super().__init__()
        self.feature_dim = feature_dim
        self.num_heads = num_heads
        self.head_dim = feature_dim // num_heads
        
        assert feature_dim % num_heads == 0, "feature_dim must be divisible by num_heads"
        
        # Query, Key, Value投影层
        self.q_proj = nn.Linear(feature_dim, feature_dim)
        self.k_proj = nn.Linear(feature_dim, feature_dim)
        self.v_proj = nn.Linear(feature_dim, feature_dim)
        
        # 输出投影层
        self.out_proj = nn.Linear(feature_dim, feature_dim)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # Layer Normalization
        self.norm1 = nn.LayerNorm(feature_dim)
        self.norm2 = nn.LayerNorm(feature_dim)
        
        # Feed Forward Network
        self.ffn = nn.Sequential(
            nn.Linear(feature_dim, feature_dim * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(feature_dim * 4, feature_dim),
            nn.Dropout(dropout)
        )
    
    def forward(self, query_features, key_value_features):
        """
        Args:
            query_features: [batch_size, feature_dim] - 查询特征
            key_value_features: [batch_size, feature_dim] - 键值特征
        """
        batch_size = query_features.size(0)
        
        # 添加序列维度
        query = query_features.unsqueeze(1)  # [batch_size, 1, feature_dim]
        key_value = key_value_features.unsqueeze(1)  # [batch_size, 1, feature_dim]
        
        # 计算Q, K, V
        Q = self.q_proj(query)  # [batch_size, 1, feature_dim]
        K = self.k_proj(key_value)  # [batch_size, 1, feature_dim]
        V = self.v_proj(key_value)  # [batch_size, 1, feature_dim]
        
        # 重塑为多头形式
        Q = Q.view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)
        
        # 计算注意力分数
        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attention_weights = F.softmax(attention_scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # 应用注意力权重
        attended_features = torch.matmul(attention_weights, V)
        
        # 重塑并投影
        attended_features = attended_features.transpose(1, 2).contiguous().view(
            batch_size, 1, self.feature_dim
        )
        attended_features = self.out_proj(attended_features).squeeze(1)
        
        # 残差连接和层归一化
        output = self.norm1(query_features + attended_features)
        
        # Feed Forward Network
        ffn_output = self.ffn(output)
        output = self.norm2(output + ffn_output)
        
        return output

class BiDirectionalCrossAttention(nn.Module):
    """双向交叉注意力"""
    
    def __init__(self, feature_dim=512, num_heads=8, dropout=0.1):
        super().__init__()
        self.attention_1_to_2 = CrossModalAttention(feature_dim, num_heads, dropout)
        self.attention_2_to_1 = CrossModalAttention(feature_dim, num_heads, dropout)
        
        # 融合层
        self.fusion_layer = nn.Sequential(
            nn.Linear(feature_dim * 2, feature_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(feature_dim, feature_dim)
        )
    
    def forward(self, features_1, features_2):
        # 双向注意力
        attended_1 = self.attention_1_to_2(features_1, features_2)
        attended_2 = self.attention_2_to_1(features_2, features_1)
        
        # 拼接并融合
        combined = torch.cat([attended_1, attended_2], dim=1)
        fused_features = self.fusion_layer(combined)
        
        return fused_features
```

### 2.2 自注意力融合

```python
class SelfAttentionFusion(nn.Module):
    """自注意力融合机制"""
    
    def __init__(self, feature_dim=512, num_heads=8, dropout=0.1):
        super().__init__()
        self.feature_dim = feature_dim
        self.num_heads = num_heads
        
        # 多头自注意力
        self.self_attention = nn.MultiheadAttention(
            embed_dim=feature_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # 位置编码
        self.pos_embedding = nn.Parameter(torch.randn(1, 2, feature_dim))
        
        # 层归一化
        self.norm = nn.LayerNorm(feature_dim)
        
        # 输出投影
        self.output_proj = nn.Linear(feature_dim * 2, feature_dim)
    
    def forward(self, features_1, features_2):
        batch_size = features_1.size(0)
        
        # 堆叠特征
        stacked_features = torch.stack([features_1, features_2], dim=1)  # [batch_size, 2, feature_dim]
        
        # 添加位置编码
        stacked_features = stacked_features + self.pos_embedding
        
        # 自注意力
        attended_features, attention_weights = self.self_attention(
            stacked_features, stacked_features, stacked_features
        )
        
        # 层归一化
        attended_features = self.norm(attended_features)
        
        # 展平并投影
        flattened = attended_features.view(batch_size, -1)
        output = self.output_proj(flattened)
        
        return output, attention_weights
```

## 3. 门控融合机制

### 3.1 基础门控网络

```python
class GatedFusion(nn.Module):
    """门控融合网络"""
    
    def __init__(self, feature_dim=512, hidden_dim=256, dropout=0.1):
        super().__init__()
        self.feature_dim = feature_dim
        
        # 门控网络
        self.gate_network = nn.Sequential(
            nn.Linear(feature_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 2),
            nn.Softmax(dim=1)
        )
        
        # 特征变换网络
        self.transform_1 = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        self.transform_2 = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # 输出层
        self.output_layer = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(feature_dim, feature_dim)
        )
    
    def forward(self, features_1, features_2):
        # 拼接特征用于门控
        combined_features = torch.cat([features_1, features_2], dim=1)
        
        # 计算门控权重
        gate_weights = self.gate_network(combined_features)  # [batch_size, 2]
        
        # 变换特征
        transformed_1 = self.transform_1(features_1)
        transformed_2 = self.transform_2(features_2)
        
        # 加权融合
        weighted_1 = transformed_1 * gate_weights[:, 0:1]
        weighted_2 = transformed_2 * gate_weights[:, 1:2]
        
        # 融合特征
        fused_features = weighted_1 + weighted_2
        
        # 输出变换
        output = self.output_layer(fused_features)
        
        return output, gate_weights

class AdaptiveGatedFusion(nn.Module):
    """自适应门控融合"""
    
    def __init__(self, feature_dim=512, num_experts=4, dropout=0.1):
        super().__init__()
        self.feature_dim = feature_dim
        self.num_experts = num_experts
        
        # 专家网络
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(feature_dim * 2, feature_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(feature_dim, feature_dim)
            ) for _ in range(num_experts)
        ])
        
        # 门控网络
        self.gating_network = nn.Sequential(
            nn.Linear(feature_dim * 2, feature_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(feature_dim, num_experts),
            nn.Softmax(dim=1)
        )
        
        # 噪声门控（用于稀疏化）
        self.noise_gate = nn.Linear(feature_dim * 2, num_experts)
        
    def forward(self, features_1, features_2, training=True):
        combined_features = torch.cat([features_1, features_2], dim=1)
        
        # 计算专家输出
        expert_outputs = []
        for expert in self.experts:
            expert_output = expert(combined_features)
            expert_outputs.append(expert_output)
        
        expert_outputs = torch.stack(expert_outputs, dim=1)  # [batch_size, num_experts, feature_dim]
        
        # 计算门控权重
        gate_weights = self.gating_network(combined_features)  # [batch_size, num_experts]
        
        # 添加噪声（训练时）
        if training:
            noise = torch.randn_like(gate_weights) * 0.1
            noise_weights = self.noise_gate(combined_features)
            gate_weights = gate_weights + noise * noise_weights
            gate_weights = F.softmax(gate_weights, dim=1)
        
        # 加权组合专家输出
        gate_weights = gate_weights.unsqueeze(-1)  # [batch_size, num_experts, 1]
        fused_output = torch.sum(expert_outputs * gate_weights, dim=1)
        
        return fused_output, gate_weights.squeeze(-1)
```

## 4. 层次化特征融合

### 4.1 多层次融合网络

```python
class HierarchicalFeatureFusion(nn.Module):
    """层次化特征融合网络"""
    
    def __init__(self, feature_dims=[512, 1024, 2048], output_dim=512, dropout=0.1):
        super().__init__()
        self.feature_dims = feature_dims
        self.output_dim = output_dim
        
        # 特征对齐层
        self.alignment_layers = nn.ModuleList([
            nn.Linear(dim, output_dim) for dim in feature_dims
        ])
        
        # 层次融合层
        self.fusion_layers = nn.ModuleList()
        
        # 从低层到高层逐步融合
        current_dim = output_dim
        for i in range(len(feature_dims) - 1):
            fusion_layer = nn.Sequential(
                nn.Linear(current_dim + output_dim, output_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(output_dim, output_dim)
            )
            self.fusion_layers.append(fusion_layer)
        
        # 最终输出层
        self.final_layer = nn.Sequential(
            nn.Linear(output_dim, output_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(output_dim, output_dim)
        )
    
    def forward(self, multi_level_features):
        """
        Args:
            multi_level_features: List of features from different levels
                                 [low_level, mid_level, high_level]
        """
        # 特征对齐
        aligned_features = []
        for i, features in enumerate(multi_level_features):
            aligned = self.alignment_layers[i](features)
            aligned_features.append(aligned)
        
        # 层次化融合
        fused_feature = aligned_features[0]  # 从最低层开始
        
        for i in range(1, len(aligned_features)):
            # 拼接当前层和之前融合的结果
            combined = torch.cat([fused_feature, aligned_features[i]], dim=1)
            # 融合
            fused_feature = self.fusion_layers[i-1](combined)
        
        # 最终输出
        output = self.final_layer(fused_feature)
        
        return output

class PyramidFeatureFusion(nn.Module):
    """金字塔特征融合网络"""
    
    def __init__(self, feature_dims=[256, 512, 1024, 2048], output_dim=512):
        super().__init__()
        self.feature_dims = feature_dims
        self.output_dim = output_dim
        
        # 1x1卷积用于维度对齐
        self.lateral_convs = nn.ModuleList([
            nn.Conv2d(dim, output_dim, 1) for dim in feature_dims
        ])
        
        # 3x3卷积用于特征融合
        self.fpn_convs = nn.ModuleList([
            nn.Conv2d(output_dim, output_dim, 3, padding=1) for _ in feature_dims
        ])
        
        # 全局平均池化
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        
        # 最终融合层
        self.final_fusion = nn.Sequential(
            nn.Linear(output_dim * len(feature_dims), output_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(output_dim * 2, output_dim)
        )
    
    def forward(self, multi_scale_features):
        """
        Args:
            multi_scale_features: List of feature maps from different scales
                                 Each feature map: [batch_size, channels, height, width]
        """
        # 自顶向下路径
        laterals = []
        for i, features in enumerate(multi_scale_features):
            lateral = self.lateral_convs[i](features)
            laterals.append(lateral)
        
        # 构建特征金字塔
        fpn_features = []
        
        # 从最高层开始
        prev_feature = laterals[-1]
        fpn_features.append(self.fpn_convs[-1](prev_feature))
        
        # 自顶向下融合
        for i in range(len(laterals) - 2, -1, -1):
            # 上采样
            upsampled = F.interpolate(
                prev_feature, 
                size=laterals[i].shape[-2:], 
                mode='bilinear', 
                align_corners=False
            )
            # 融合
            fused = laterals[i] + upsampled
            fpn_feature = self.fpn_convs[i](fused)
            fpn_features.insert(0, fpn_feature)
            prev_feature = fused
        
        # 全局池化并拼接
        pooled_features = []
        for fpn_feature in fpn_features:
            pooled = self.global_pool(fpn_feature).flatten(1)
            pooled_features.append(pooled)
        
        # 拼接所有尺度的特征
        combined_features = torch.cat(pooled_features, dim=1)
        
        # 最终融合
        output = self.final_fusion(combined_features)
        
        return output
```

## 5. 自适应权重学习

### 5.1 动态权重网络

```python
class DynamicWeightNetwork(nn.Module):
    """动态权重网络"""
    
    def __init__(self, feature_dim=512, context_dim=128, num_modalities=2, dropout=0.1):
        super().__init__()
        self.feature_dim = feature_dim
        self.context_dim = context_dim
        self.num_modalities = num_modalities
        
        # 上下文编码器
        self.context_encoder = nn.Sequential(
            nn.Linear(feature_dim * num_modalities, context_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(context_dim * 2, context_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # 权重生成器
        self.weight_generator = nn.Sequential(
            nn.Linear(context_dim, context_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(context_dim // 2, num_modalities),
            nn.Softmax(dim=1)
        )
        
        # 特征变换器
        self.feature_transformers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(feature_dim, feature_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(feature_dim, feature_dim)
            ) for _ in range(num_modalities)
        ])
        
        # 融合后处理
        self.post_fusion = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(feature_dim, feature_dim)
        )
    
    def forward(self, features_list):
        """
        Args:
            features_list: List of features from different modalities
                          Each feature: [batch_size, feature_dim]
        """
        # 拼接所有特征用于上下文编码
        all_features = torch.cat(features_list, dim=1)
        
        # 编码上下文
        context = self.context_encoder(all_features)
        
        # 生成动态权重
        weights = self.weight_generator(context)  # [batch_size, num_modalities]
        
        # 变换特征
        transformed_features = []
        for i, features in enumerate(features_list):
            transformed = self.feature_transformers[i](features)
            transformed_features.append(transformed)
        
        # 加权融合
        weighted_features = []
        for i, features in enumerate(transformed_features):
            weighted = features * weights[:, i:i+1]
            weighted_features.append(weighted)
        
        # 求和融合
        fused_features = torch.stack(weighted_features, dim=0).sum(dim=0)
        
        # 后处理
        output = self.post_fusion(fused_features)
        
        return output, weights

class QualityAwareFusion(nn.Module):
    """质量感知融合网络"""
    
    def __init__(self, feature_dim=512, quality_dim=64, dropout=0.1):
        super().__init__()
        self.feature_dim = feature_dim
        self.quality_dim = quality_dim
        
        # 质量评估网络
        self.quality_assessor = nn.Sequential(
            nn.Linear(feature_dim, quality_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(quality_dim * 2, quality_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(quality_dim, 1),
            nn.Sigmoid()  # 质量分数在[0,1]范围内
        )
        
        # 不确定性估计网络
        self.uncertainty_estimator = nn.Sequential(
            nn.Linear(feature_dim, quality_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(quality_dim, 1),
            nn.Sigmoid()  # 不确定性在[0,1]范围内
        )
        
        # 特征增强网络
        self.feature_enhancer = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(feature_dim, feature_dim)
        )
        
        # 融合网络
        self.fusion_network = nn.Sequential(
            nn.Linear(feature_dim * 2, feature_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(feature_dim, feature_dim)
        )
    
    def forward(self, features_1, features_2):
        # 评估特征质量
        quality_1 = self.quality_assessor(features_1)
        quality_2 = self.quality_assessor(features_2)
        
        # 估计不确定性
        uncertainty_1 = self.uncertainty_estimator(features_1)
        uncertainty_2 = self.uncertainty_estimator(features_2)
        
        # 计算可靠性权重
        reliability_1 = quality_1 * (1 - uncertainty_1)
        reliability_2 = quality_2 * (1 - uncertainty_2)
        
        # 归一化权重
        total_reliability = reliability_1 + reliability_2 + 1e-8
        weight_1 = reliability_1 / total_reliability
        weight_2 = reliability_2 / total_reliability
        
        # 增强特征
        enhanced_1 = self.feature_enhancer(features_1)
        enhanced_2 = self.feature_enhancer(features_2)
        
        # 加权特征
        weighted_1 = enhanced_1 * weight_1
        weighted_2 = enhanced_2 * weight_2
        
        # 拼接并融合
        combined = torch.cat([weighted_1, weighted_2], dim=1)
        fused_output = self.fusion_network(combined)
        
        return fused_output, {
            'quality_1': quality_1,
            'quality_2': quality_2,
            'uncertainty_1': uncertainty_1,
            'uncertainty_2': uncertainty_2,
            'weight_1': weight_1,
            'weight_2': weight_2
        }
```

## 6. 完整的VQA融合模型

### 6.1 统一融合架构

```python
class UnifiedVQAFusionModel(nn.Module):
    """统一的VQA特征融合模型"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # 特征提取器
        self.spatial_extractor_1 = self._create_extractor(
            config.spatial_backbone_1, config.feature_dim
        )
        self.spatial_extractor_2 = self._create_extractor(
            config.spatial_backbone_2, config.feature_dim
        )
        
        # 时序特征提取器（如果需要）
        if config.use_temporal:
            self.temporal_extractor = self._create_temporal_extractor(config)
        
        # 融合策略选择
        self.fusion_strategy = self._create_fusion_strategy(config)
        
        # 质量预测头
        self.quality_head = nn.Sequential(
            nn.Linear(config.feature_dim, config.feature_dim // 2),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.feature_dim // 2, config.feature_dim // 4),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.feature_dim // 4, 1)
        )
    
    def _create_extractor(self, backbone_type, feature_dim):
        """创建特征提取器"""
        if backbone_type == 'efficientnet':
            return EfficientNetExtractor(output_dim=feature_dim)
        elif backbone_type == 'vit':
            return ViTExtractor(output_dim=feature_dim)
        elif backbone_type == 'convnext':
            return ConvNeXtExtractor(output_dim=feature_dim)
        else:
            raise ValueError(f"Unsupported backbone type: {backbone_type}")
    
    def _create_temporal_extractor(self, config):
        """创建时序特征提取器"""
        # 这里可以添加TSM、TEA等时序建模方法
        return nn.Identity()  # 简化实现
    
    def _create_fusion_strategy(self, config):
        """创建融合策略"""
        strategy = config.fusion_strategy
        feature_dim = config.feature_dim
        
        if strategy == 'cross_attention':
            return BiDirectionalCrossAttention(feature_dim)
        elif strategy == 'self_attention':
            return SelfAttentionFusion(feature_dim)
        elif strategy == 'gated':
            return GatedFusion(feature_dim)
        elif strategy == 'adaptive_gated':
            return AdaptiveGatedFusion(feature_dim)
        elif strategy == 'dynamic_weight':
            return DynamicWeightNetwork(feature_dim)
        elif strategy == 'quality_aware':
            return QualityAwareFusion(feature_dim)
        else:
            raise ValueError(f"Unsupported fusion strategy: {strategy}")
    
    def forward(self, x1, x2=None, temporal_data=None):
        """
        Args:
            x1: Input for first extractor
            x2: Input for second extractor (optional)
            temporal_data: Temporal sequence data (optional)
        """
        # 提取空间特征
        features_1 = self.spatial_extractor_1(x1)
        
        if x2 is not None:
            features_2 = self.spatial_extractor_2(x2)
        else:
            # 如果只有一个输入，使用不同的处理方式
            features_2 = self.spatial_extractor_2(x1)
        
        # 时序特征（如果需要）
        if hasattr(self, 'temporal_extractor') and temporal_data is not None:
            temporal_features = self.temporal_extractor(temporal_data)
            # 可以将时序特征与空间特征结合
            features_1 = features_1 + temporal_features
        
        # 特征融合
        if isinstance(self.fusion_strategy, (BiDirectionalCrossAttention, SelfAttentionFusion)):
            fused_features, attention_weights = self.fusion_strategy(features_1, features_2)
            fusion_info = {'attention_weights': attention_weights}
        elif isinstance(self.fusion_strategy, (GatedFusion, AdaptiveGatedFusion)):
            fused_features, gate_weights = self.fusion_strategy(features_1, features_2)
            fusion_info = {'gate_weights': gate_weights}
        elif isinstance(self.fusion_strategy, DynamicWeightNetwork):
            fused_features, weights = self.fusion_strategy([features_1, features_2])
            fusion_info = {'dynamic_weights': weights}
        elif isinstance(self.fusion_strategy, QualityAwareFusion):
            fused_features, quality_info = self.fusion_strategy(features_1, features_2)
            fusion_info = quality_info
        else:
            fused_features = self.fusion_strategy(features_1, features_2)
            fusion_info = {}
        
        # 质量预测
        quality_score = self.quality_head(fused_features)
        
        return {
            'quality_score': quality_score,
            'fused_features': fused_features,
            'fusion_info': fusion_info
        }
```

### 6.2 配置类

```python
class VQAFusionConfig:
    """VQA融合模型配置"""
    
    def __init__(self):
        # 基础配置
        self.feature_dim = 512
        self.dropout = 0.1
        
        # 骨干网络配置
        self.spatial_backbone_1 = 'efficientnet'  # efficientnet, vit, convnext
        self.spatial_backbone_2 = 'vit'
        
        # 时序配置
        self.use_temporal = False
        self.temporal_method = 'tsm'  # tsm, tea, slowfast
        
        # 融合策略配置
        self.fusion_strategy = 'cross_attention'  # cross_attention, self_attention, gated, etc.
        
        # 注意力配置
        self.num_heads = 8
        self.attention_dropout = 0.1
        
        # 门控配置
        self.num_experts = 4
        self.gate_hidden_dim = 256
        
        # 训练配置
        self.learning_rate = 1e-4
        self.weight_decay = 1e-5
        self.batch_size = 32
        self.num_epochs = 100
        
        # 损失函数配置
        self.loss_type = 'mse'  # mse, l1, huber
        self.loss_weights = {
            'quality': 1.0,
            'attention_reg': 0.01,
            'sparsity_reg': 0.001
        }
```

## 7. 训练和推理代码

### 7.1 训练循环

```python
import torch.optim as optim
from torch.utils.data import DataLoader

class VQATrainer:
    """VQA模型训练器"""
    
    def __init__(self, model, config, train_loader, val_loader):
        self.model = model
        self.config = config
        self.train_loader = train_loader
        self.val_loader = val_loader
        
        # 优化器
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        
        # 学习率调度器
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=config.num_epochs
        )
        
        # 损失函数
        self.criterion = self._create_criterion(config.loss_type)
        
        # 设备
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
    
    def _create_criterion(self, loss_type):
        """创建损失函数"""
        if loss_type == 'mse':
            return nn.MSELoss()
        elif loss_type == 'l1':
            return nn.L1Loss()
        elif loss_type == 'huber':
            return nn.SmoothL1Loss()
        else:
            raise ValueError(f"Unsupported loss type: {loss_type}")
    
    def train_epoch(self):
        """训练一个epoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0
        
        for batch_idx, (data1, data2, targets) in enumerate(self.train_loader):
            data1, data2, targets = data1.to(self.device), data2.to(self.device), targets.to(self.device)
            
            # 前向传播
            self.optimizer.zero_grad()
            outputs = self.model(data1, data2)
            
            # 计算损失
            quality_loss = self.criterion(outputs['quality_score'], targets)
            
            # 添加正则化项
            total_loss_batch = quality_loss * self.config.loss_weights['quality']
            
            # 注意力正则化（如果有）
            if 'attention_weights' in outputs['fusion_info']:
                attention_reg = self._attention_regularization(
                    outputs['fusion_info']['attention_weights']
                )
                total_loss_batch += attention_reg * self.config.loss_weights['attention_reg']
            
            # 稀疏性正则化（如果有）
            if 'gate_weights' in outputs['fusion_info']:
                sparsity_reg = self._sparsity_regularization(
                    outputs['fusion_info']['gate_weights']
                )
                total_loss_batch += sparsity_reg * self.config.loss_weights['sparsity_reg']
            
            # 反向传播
            total_loss_batch.backward()
            
            # 梯度裁剪
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # 更新参数
            self.optimizer.step()
            
            total_loss += total_loss_batch.item()
            num_batches += 1
        
        return total_loss / num_batches
    
    def validate(self):
        """验证模型"""
        self.model.eval()
        total_loss = 0
        predictions = []
        targets_list = []
        
        with torch.no_grad():
            for data1, data2, targets in self.val_loader:
                data1, data2, targets = data1.to(self.device), data2.to(self.device), targets.to(self.device)
                
                outputs = self.model(data1, data2)
                loss = self.criterion(outputs['quality_score'], targets)
                
                total_loss += loss.item()
                predictions.extend(outputs['quality_score'].cpu().numpy())
                targets_list.extend(targets.cpu().numpy())
        
        # 计算相关系数
        predictions = np.array(predictions)
        targets_list = np.array(targets_list)
        
        from scipy.stats import pearsonr, spearmanr
        plcc = pearsonr(predictions.flatten(), targets_list.flatten())[0]
        srcc = spearmanr(predictions.flatten(), targets_list.flatten())[0]
        
        return total_loss / len(self.val_loader), plcc, srcc
    
    def _attention_regularization(self, attention_weights):
        """注意力正则化"""
        # 鼓励注意力分布的多样性
        entropy = -torch.sum(attention_weights * torch.log(attention_weights + 1e-8), dim=-1)
        return -torch.mean(entropy)  # 负熵，鼓励高熵
    
    def _sparsity_regularization(self, gate_weights):
        """稀疏性正则化"""
        # 鼓励门控权重的稀疏性
        return torch.mean(torch.sum(gate_weights ** 2, dim=-1))
    
    def train(self):
        """完整训练流程"""
        best_srcc = 0
        
        for epoch in range(self.config.num_epochs):
            # 训练
            train_loss = self.train_epoch()
            
            # 验证
            val_loss, plcc, srcc = self.validate()
            
            # 更新学习率
            self.scheduler.step()
            
            # 保存最佳模型
            if srcc > best_srcc:
                best_srcc = srcc
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'epoch': epoch,
                    'best_srcc': best_srcc,
                    'config': self.config
                }, 'best_model.pth')
            
            print(f'Epoch {epoch+1}/{self.config.num_epochs}:')
            print(f'  Train Loss: {train_loss:.4f}')
            print(f'  Val Loss: {val_loss:.4f}')
            print(f'  PLCC: {plcc:.4f}, SRCC: {srcc:.4f}')
            print(f'  Best SRCC: {best_srcc:.4f}')
            print('-' * 50)
```

### 7.2 使用示例

```python
# 创建配置
config = VQAFusionConfig()
config.fusion_strategy = 'cross_attention'
config.spatial_backbone_1 = 'efficientnet'
config.spatial_backbone_2 = 'vit'

# 创建模型
model = UnifiedVQAFusionModel(config)

# 创建数据加载器（需要根据实际数据集实现）
# train_loader = DataLoader(...)
# val_loader = DataLoader(...)

# 创建训练器
# trainer = VQATrainer(model, config, train_loader, val_loader)

# 开始训练
# trainer.train()

# 推理示例
model.eval()
with torch.no_grad():
    # 假设有两个输入图像
    input1 = torch.randn(1, 3, 224, 224)
    input2 = torch.randn(1, 3, 224, 224)
    
    # 前向传播
    outputs = model(input1, input2)
    
    quality_score = outputs['quality_score']
    fusion_info = outputs['fusion_info']
    
    print(f"Predicted quality score: {quality_score.item():.4f}")
    
    # 如果使用注意力机制，可以可视化注意力权重
    if 'attention_weights' in fusion_info:
        attention_weights = fusion_info['attention_weights']
        print(f"Attention weights shape: {attention_weights.shape}")
```

## 8. 总结

本实现方案提供了完整的特征融合解决方案，包括：

1. **模块化设计**：每个组件都可以独立使用和测试
2. **多种融合策略**：支持注意力、门控、层次化等多种融合方法
3. **灵活配置**：通过配置类轻松切换不同的网络和策略
4. **完整训练流程**：包含训练、验证和推理的完整代码
5. **可扩展性**：易于添加新的特征提取器和融合策略

通过这些实现，您可以根据具体需求选择合适的融合策略，并进行端到端的训练和优化。